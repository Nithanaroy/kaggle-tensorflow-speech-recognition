{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Recognition using Tensorflow\n",
    "\n",
    "This approach uses CNN to build a classifier for audio inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from tensorflow.python.client import timeline # for profiling\n",
    "from math import ceil\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_data(\"../data/vectorized/90_10_split_from_train/train_sounds.h5\", \"../data/vectorized/90_10_split_from_train/test_sounds.h5\", \"../data/vectorized/90_10_split_from_train/classes_sounds.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 58248\n",
      "number of test examples = 6473\n",
      "X_train shape: (58248, 16000, 1, 1)\n",
      "Y_train shape: (58248, 30)\n",
      "X_test shape: (6473, 16000, 1, 1)\n",
      "Y_test shape: (6473, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig\n",
    "X_test = X_test_orig\n",
    "Y_train = convert_to_one_hot(Y_train_orig, classes)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, classes)\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input placeholders\n",
    "\n",
    "Tensorflow placeholders for X and Y. These will be dynamically set during batch G.D at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_l, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "\n",
    "    Arguments:\n",
    "    n_l -- scalar, length of the audio vector\n",
    "    n_y -- scalar, number of classes\n",
    "\n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_l] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_l, 1, 1), name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_y), name=\"Y\")\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = create_placeholders(500, 20)\n",
    "# print (\"X = \" + str(X))\n",
    "# print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters\n",
    "\n",
    "With tensorflow we only need to initialize parameters for Conv layers. Fully connected layers' paramaters are completed handled by the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "    W1 : [4, 1, 1, 8]\n",
    "    W2 : [2, 1, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "\n",
    "    tf.set_random_seed(1)\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", [4,1,1,8], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", [2,1,8,16], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "\n",
    "    parameters = {\"W1\": W1, \"W2\": W2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# with tf.Session() as sess_test:\n",
    "#     parameters = initialize_parameters()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess_test.run(init)\n",
    "#     print(\"W1 = \" + str(parameters[\"W1\"].eval()[0,0,0]))\n",
    "#     print(\"W2 = \" + str(parameters[\"W2\"].eval()[0,0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\n",
    "Following are the parameters for all the layers:\n",
    "    - Conv2D: stride 1, padding is \"SAME\"\n",
    "    - ReLU\n",
    "    - Max pool: 8 by 1 filter size and an 8 by 1 stride, padding is \"SAME\"\n",
    "    - Conv2D: stride 1, padding is \"SAME\"\n",
    "    - ReLU\n",
    "    - Max pool: 4 by 1 filter size and a 4 by 1 stride, padding is \"SAME\"\n",
    "    - Flatten the previous output.\n",
    "    - FULLYCONNECTED (FC) layer: outputs 30 classes one for each audio utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing parameters \"W1\", \"W2\"\n",
    "    the shapes are given in initialize_parameters\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"\n",
    "#     W1 = parameters['W1']\n",
    "#     W2 = parameters['W2']\n",
    "    regularizer1 = tf.contrib.layers.l2_regularizer(scale=0.001)\n",
    "    regularizer2 = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "    regularizer3 = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "    regularizer4 = tf.contrib.layers.l2_regularizer(scale=10.0)\n",
    "#     regularizer = None\n",
    "\n",
    "    Z1 = tf.layers.conv2d(X, 8, (4,1), strides = [1,1], padding = 'SAME', kernel_regularizer = regularizer2, name=\"z1\")\n",
    "    A1 = tf.nn.relu(Z1, name=\"a1\")\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,1,1], strides = [1,8,1,1], padding = 'SAME', name=\"p1\")\n",
    "    Z2 = tf.layers.conv2d(P1, 16, (2, 1), strides = [1,1], padding = 'SAME', kernel_regularizer = regularizer2, name=\"z2\")\n",
    "    A2 = tf.nn.relu(Z2, name=\"a2\")\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,4,1,1], strides = [1,4,1,1], padding = 'SAME', name=\"p2\")\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, 30, activation_fn=None, weights_regularizer = regularizer2)\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# with tf.Session() as sess:\n",
    "#     np.random.seed(1)\n",
    "#     X, Y = create_placeholders(64, 5)\n",
    "#     parameters = initialize_parameters()\n",
    "#     Z3 = forward_propagation(X, parameters)\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "#     a = sess.run(Z3, {X: np.random.randn(2,64,1,1), Y: np.random.randn(2,5)})\n",
    "#     print(\"Z3 = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost\n",
    "\n",
    "Using the last layer Z3, compute softmax and J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "\n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (30, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "\n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y, name=\"L\"), name=\"J\")\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# with tf.Session() as sess:\n",
    "#     np.random.seed(1)\n",
    "#     X, Y = create_placeholders(64, 30)\n",
    "#     parameters = initialize_parameters()\n",
    "#     Z3 = forward_propagation(X, parameters)\n",
    "#     cost = compute_cost(Z3, Y)\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "#     a = sess.run(cost, {X: np.random.randn(4,64,1,1), Y: np.random.randn(4,30)})\n",
    "#     print(\"cost = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(X_train, Y_train, Z3, X, Y, minibatch_size = 64, percent_data = 100, print_progress = True):\n",
    "    \"\"\"\n",
    "    percent_data-- approximate max % amount of data to consider while computing accuracy\n",
    "    \"\"\"\n",
    "    predict_op = tf.argmax(Z3, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    num_minibatches = 0\n",
    "    acc_accuracy = 0\n",
    "    total_minibatches = ceil(X_train.shape[0] / float(minibatch_size))\n",
    "    max_num_minibatches = total_minibatches * percent_data / 100.0\n",
    "    minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "    for minibatch in minibatches:\n",
    "        (minibatch_X, minibatch_Y) = minibatch\n",
    "        acc_accuracy += accuracy.eval({X: minibatch_X, Y: minibatch_Y})\n",
    "        num_minibatches += 1\n",
    "        \n",
    "        if print_progress and num_minibatches % 25 == 0:\n",
    "            print(\"%s: Accuracy after %ith batch: %f\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), num_minibatches, acc_accuracy / num_minibatches))\n",
    "        if num_minibatches >= max_num_minibatches:\n",
    "            break\n",
    "\n",
    "    train_accuracy = acc_accuracy / num_minibatches\n",
    "    # print(\"Accuracy:\", train_accuracy)\n",
    "\n",
    "    return train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Helper for (cost, test accuracy, train accuracy) VS (# iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_test_train(num_epochs, costs, test_accs, title = \"\"):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    t = np.arange(num_epochs)\n",
    "    ax1.plot(t, costs, 'b-')\n",
    "    ax1.set_xlabel('iterations (per tens)')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax1.set_ylabel('train cost', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(t, training_accs, 'r-')\n",
    "#     ax2.set_ylabel('train accuracy', color='r')\n",
    "#     ax2.tick_params('y', colors='r')\n",
    "    \n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.plot(t, test_accs, 'm-')\n",
    "    ax3.set_ylabel('test cost', color='m')\n",
    "    ax3.tick_params('y', colors='m')\n",
    "#     ax3.spines['right'].set_position(('axes', 1.2))\n",
    "\n",
    "    fig.tight_layout()\n",
    "#     fig.subplots_adjust(right=0.75) # add space on the right for y3 axis\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Connects all the functions and sets up training with mini batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # to be able to rerun the model without overwriting tf variables\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "\n",
    "# Start an interactive session\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "run_metadata = tf.RunMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(X_train, Y_train, learning_rate):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 16000, 1, 1)\n",
    "    Y_train -- test set, of shape (None, n_y = 30)\n",
    "    X_test -- training set, of shape (None, 16000, 1, 1)\n",
    "    Y_test -- test set, of shape (None, n_y = 30)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "\n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    tf.set_random_seed(1) # to keep results consistent (tensorflow seed)\n",
    "    (m, n_l, _, __) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "\n",
    "    X, Y = create_placeholders(n_l, n_y)\n",
    "    Z3 = forward_propagation(X)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Using AdamOptimizer to minimize the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, name=\"adam\").minimize(cost, name=\"adam_minimize\")\n",
    "    \n",
    "    streaming_cost, streaming_cost_update = tf.contrib.metrics.streaming_mean(cost)\n",
    "    streaming_cost_scalar = tf.summary.scalar('streaming_cost', streaming_cost)\n",
    "    # tf.summary.scalar('current_cost', cost)\n",
    "    # summary = tf.summary.merge_all()\n",
    "\n",
    "    return Z3, X, Y, cost, optimizer, streaming_cost_update, streaming_cost_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, Y_train, X_test, Y_test, X, Y, \n",
    "              cost, optimizer, learning_rate = 0.011, minibatch_size = 64, num_epochs = 100, print_cost = True):\n",
    "    \n",
    "    seed = 3 # to keep results consistent (numpy seed)\n",
    "    m = X_train.shape[0]\n",
    "    num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "    training_costs = []\n",
    "    testing_costs = []\n",
    "    \n",
    "    # Run the initialization\n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        minibatch_cost = 0.\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "            # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "            _, temp_cost, _ = sess.run([optimizer, cost, streaming_cost_update], feed_dict = {X: minibatch_X, Y: minibatch_Y}) \n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "\n",
    "        # Print the cost every # epochs\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print(\"%s: Training cost after epoch %i: %f\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), epoch, minibatch_cost))\n",
    "            training_costs.append(minibatch_cost)\n",
    "            testing_summary, temp_cost = sess.run([streaming_cost_scalar, cost], feed_dict = {X: X_test, Y: Y_test})\n",
    "            testing_costs.append(temp_cost)\n",
    "            \n",
    "            # Write to tensorboard\n",
    "            training_summary = sess.run(streaming_cost_scalar)\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "        \n",
    "    plot_cost_test_train(num_epochs, training_costs, testing_costs, \"Learning rate = %s\" % learning_rate)\n",
    "    return training_costs, testing_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "Z3, X, Y, cost, optimizer, streaming_cost_update, streaming_cost_scalar = create_model(X_train, \n",
    "                                                                                       Y_train, \n",
    "                                                                                       learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver() # create a saver for saving variables to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tensor Board Settings after creating the graph\n",
    "\n",
    "RUN_NAME = \"N: L2 0.01, alp 0.001, ep 300\"\n",
    "training_writer = tf.summary.FileWriter(\"../logs/{}/training\".format(RUN_NAME), sess.graph)\n",
    "testing_writer = tf.summary.FileWriter(\"../logs/{}/testing\".format(RUN_NAME), sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08 03:33:20: Training cost after epoch 0: 3.220568\n",
      "2018-01-08 03:47:55: Training cost after epoch 10: 2.005192\n",
      "2018-01-08 04:02:34: Training cost after epoch 20: 1.583197\n",
      "2018-01-08 04:17:15: Training cost after epoch 30: 1.314128\n",
      "2018-01-08 04:31:54: Training cost after epoch 40: 1.128783\n",
      "2018-01-08 04:46:36: Training cost after epoch 50: 0.986306\n",
      "2018-01-08 05:01:17: Training cost after epoch 60: 0.869110\n",
      "2018-01-08 05:15:57: Training cost after epoch 70: 0.775745\n",
      "2018-01-08 05:30:37: Training cost after epoch 80: 0.694340\n",
      "2018-01-08 05:45:18: Training cost after epoch 90: 0.638192\n",
      "2018-01-08 05:59:58: Training cost after epoch 100: 0.590813\n",
      "2018-01-08 06:14:39: Training cost after epoch 110: 0.550957\n",
      "2018-01-08 06:29:19: Training cost after epoch 120: 0.505131\n",
      "2018-01-08 06:43:59: Training cost after epoch 130: 0.441572\n",
      "2018-01-08 06:58:39: Training cost after epoch 140: 0.474589\n",
      "2018-01-08 07:13:20: Training cost after epoch 150: 0.390009\n",
      "2018-01-08 07:28:01: Training cost after epoch 160: 0.374300\n",
      "2018-01-08 07:42:41: Training cost after epoch 170: 0.380546\n",
      "2018-01-08 07:57:22: Training cost after epoch 180: 0.402215\n",
      "2018-01-08 08:12:02: Training cost after epoch 190: 0.292346\n",
      "2018-01-08 08:26:42: Training cost after epoch 200: 0.328485\n",
      "2018-01-08 08:41:22: Training cost after epoch 210: 0.256061\n",
      "2018-01-08 08:56:02: Training cost after epoch 220: 0.265470\n",
      "2018-01-08 09:10:42: Training cost after epoch 230: 0.280139\n",
      "2018-01-08 09:25:23: Training cost after epoch 240: 0.332993\n",
      "2018-01-08 09:40:04: Training cost after epoch 250: 0.316078\n",
      "2018-01-08 09:54:32: Training cost after epoch 260: 0.218934\n",
      "2018-01-08 10:09:10: Training cost after epoch 270: 0.261323\n",
      "2018-01-08 10:23:49: Training cost after epoch 280: 0.245845\n",
      "2018-01-08 10:38:30: Training cost after epoch 290: 0.194453\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (300,) and (30,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4baf1565e085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m run_model(X_train, Y_train, X_test, Y_test, X, Y, \n\u001b[1;32m      2\u001b[0m           \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           minibatch_size = 256, num_epochs = 300)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-b44258a0cbfb>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(X_train, Y_train, X_test, Y_test, X, Y, cost, optimizer, learning_rate, minibatch_size, num_epochs, print_cost)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtesting_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mplot_cost_test_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning rate = %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_costs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e978d0602e4d>\u001b[0m in \u001b[0;36mplot_cost_test_train\u001b[0;34m(num_epochs, costs, test_accs, title)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iterations (per tens)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Make the y-axis label, ticks and tick labels match the line color.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nipasuma/Projects/kaggle-tensorflow-speech-recognition/src/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1716\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nipasuma/Projects/kaggle-tensorflow-speech-recognition/src/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nipasuma/Projects/kaggle-tensorflow-speech-recognition/src/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nipasuma/Projects/kaggle-tensorflow-speech-recognition/src/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nipasuma/Projects/kaggle-tensorflow-speech-recognition/src/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 243\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (300,) and (30,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x31fc590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(X_train, Y_train, X_test, Y_test, X, Y, \n",
    "          cost, optimizer, learning_rate = learning_rate,\n",
    "          minibatch_size = 256, num_epochs = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save profiling data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Timeline object, and write it to a json file\n",
    "def save_profiling_data():\n",
    "    fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n",
    "    chrome_trace = fetched_timeline.generate_chrome_trace_format()\n",
    "    time_id = int(time())\n",
    "    with open('../experiments/timeline_s_256b_120e_gpu0_l2_%s.json' % (time_id,), 'w') as f:\n",
    "        f.write(chrome_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = saver.save(sess, \"../saved_models/l2_10_300e_256b_l2-reg-0.01_alpha-0.001.ckpt\")\n",
    "\n",
    "# meta_graph_def = tf.train.export_meta_graph(filename='../saved_models/my-cnn-tf-model.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.list_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08 11:00:02: Accuracy after 25th batch: 0.953906\n",
      "2018-01-08 11:00:04: Accuracy after 50th batch: 0.955859\n",
      "2018-01-08 11:00:07: Accuracy after 75th batch: 0.954427\n",
      "2018-01-08 11:00:09: Accuracy after 100th batch: 0.954102\n",
      "2018-01-08 11:00:11: Accuracy after 125th batch: 0.955125\n",
      "2018-01-08 11:00:13: Accuracy after 150th batch: 0.956042\n",
      "2018-01-08 11:00:16: Accuracy after 175th batch: 0.955826\n",
      "2018-01-08 11:00:18: Accuracy after 200th batch: 0.955566\n",
      "2018-01-08 11:00:20: Accuracy after 225th batch: 0.955573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95541977071971229"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train accuracy\n",
    "model_accuracy(X_train, Y_train, Z3, X, Y, minibatch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08 11:00:53: Accuracy after 25th batch: 0.267188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26850220675651842"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "model_accuracy(X_test, Y_test, Z3, X, Y, minibatch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "- Convert audio file to vector and reshape\n",
    "- Do forward prop\n",
    "- Find the maximal class\n",
    "- Remap index to class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(audio_file, Z3):\n",
    "    ra = load_wav_file(os.path.abspath(audio_file))\n",
    "    x = ra.reshape(1, ra.shape[0], 1, 1)\n",
    "    y_hat = tf.argmax(Z3, 1)\n",
    "    prediction = sess.run(y_hat, feed_dict = {X: x})\n",
    "    return classes[prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference(\"../data/train/audio/bed/0a7c2a8d_nohash_0.wav\", Z3)) # bed\n",
    "print(inference(\"../data/train/audio/down/0a7c2a8d_nohash_0.wav\", Z3)) # down\n",
    "print(inference(\"../data/test/audio/clip_0000adecb.wav\", Z3)) # happy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver() # create a saver for saving variables to disk\n",
    "# saver.restore(sess, \"../saved_models/trained_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
